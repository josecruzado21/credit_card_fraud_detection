{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advised-convention",
   "metadata": {},
   "source": [
    "# Fraud Detection in Credit Card Payments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-bullet",
   "metadata": {},
   "source": [
    "The data used in the following notebook was obtained from Kaggle (Dal Pozzolo et al 2015). Link to the dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud/data\n",
    "\n",
    "According to the dictionary, to protect the identity, the variables in the data set are the consequence of a dimensionality reduction process (PCA). The time variable represent the number of seconds elapsed between the transaction and the first transaction in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-clock",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "In case you cloned this repository from Amazon Sagemaker, the best way to obtain the data is to run the following lines:\n",
    "\n",
    "```\n",
    "!rm creditcard.csv\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c534768_creditcardfraud/creditcardfraud.zip\n",
    "!unzip creditcardfraud\n",
    "!rm creditcardfraud.zip\n",
    "```\n",
    "\n",
    "Because creditcard.csv was uploaded using `lfs`, When this repository is cloned from Sagemaker, the creditcard.csv file will contain only pointers.\n",
    "\n",
    "If you clone this repo locally and `lfs` is configured, the dataset will be correcly downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-celebrity",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civic-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm creditcard.csv\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c534768_creditcardfraud/creditcardfraud.zip\n",
    "!unzip creditcardfraud\n",
    "!rm creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intermediate-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains {} observations and {} features\".format(data.shape[0],data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-superintendent",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "* V1-V28: The principal componentes obtained by PCA. The original features cannot be disclosed for confidentiality reasons.\n",
    "* Time: Seconds elapsed between the transaction and the first transaction in the dataset.\n",
    "* Amount: The amount of the transaction.\n",
    "* Class: Takes the value of **1** if it was a fraudulent transaction and **0** otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-grave",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "The data contains two days of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Time.plot(kind='density');\n",
    "data.Time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-second",
   "metadata": {},
   "source": [
    "#### PCA components\n",
    "\n",
    "As expected, all the components of the PCA has zero correlation with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.loc[:,data.columns.str.startswith('V')].corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-enemy",
   "metadata": {},
   "source": [
    "#### Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-czech",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-wireless",
   "metadata": {},
   "source": [
    "The dataset is extremely imbalanced, with only 0.17% of the observations labelled as fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of fraudulent transactions in the dataset: {}\".format(data.Class.value_counts()[1]))\n",
    "print(\"Proportion of fraudulent transactions in the dataset: {}%\".format((data.Class.value_counts()[1]/len(data.Class))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-cathedral",
   "metadata": {},
   "source": [
    "## Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-martial",
   "metadata": {},
   "source": [
    "Our task is to detect the as many fraudulent transactions as possible. Provided that our data is extremely imbalanced, with more that 99% of the observations labelled as non-fraudulent, algorithms will generally tend to predict all transactions to be on the majority class. For this reason, we should focus on minimizing False Negatives and maximizing True Positives (maximizing Recall).\n",
    "\n",
    "\\begin{align}\n",
    "Recall=\\frac{TP}{TP+FN}\n",
    "\\end{align}\n",
    "\n",
    "Performing cross validation to estimate the performance (recall) on the test set would not give accurate results. The reason for this is the low proportion of fraudulent transactions, which causes a great variability on the amout of class-1 observations used for training and test in each validation set. \n",
    "\n",
    "As we only have 492 fraudulent data points, training an algorithm with 400 of them and testing on 92 may generate very different results as training on 250 of them and testing on the rest.\n",
    "\n",
    "In order to mantain the proportion of class-1 points in the training and test set, we will perform a stratified sampling to divide the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(by='Class').sample(frac=0.7).Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrative-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(data,fraction):\n",
    "    \"\"\"\n",
    "    Input: the dataset to split into training and test\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    The function returns X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \"\"\"\n",
    "    train=data.iloc[data.groupby(by='Class').sample(frac=fraction).index]\n",
    "    test=data.drop(train.index,axis=0)\n",
    "    \n",
    "    X_train=train.drop('Class',axis=1)\n",
    "    X_test=test.drop('Class',axis=1)\n",
    "    y_train=train.Class\n",
    "    y_test=test.Class\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-xerox",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-criterion",
   "metadata": {},
   "source": [
    "#### Not accounting for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores=[]\n",
    "auc_scores=[]\n",
    "for i in range(20):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data)\n",
    "    log_reg=LogisticRegression(max_iter=1000).fit(X_train,y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    recall_scores.append(recall_score(y_test,y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test,log_reg.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"The Logistic Regression average recall score without accounting for class imbalance is: {}\".format(sum(recall_scores)/len(recall_scores)))\n",
    "print(\"The Logistic Regression averace AUC score without accounting for class imbalance is: {}\".format(sum(auc_scores)/len(auc_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-knight",
   "metadata": {},
   "source": [
    "Is important to highlight that the 20 recall scores found above are not independent. \n",
    "\n",
    "Some training observations in the training set (or test) are the same among different repetitions. However, using 20 different iterations with only 492 class-1 observations makes highly probable that all the observations are used as training and test data points at least once. The latter occurs because we always select the same proportion of the 492 observations as training and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-dover",
   "metadata": {},
   "source": [
    "An example of a confusion matrix in one of the 20 repetitions estimated above\n",
    "\n",
    "|Confusion Matrix|y_true=1|y_true=0|\n",
    "|----------------|--------|--------|\n",
    "|**y_predicted=1**|TP=84|FP=16|\n",
    "|**y_predicted=0**|FN=64|TN=85,279|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-family",
   "metadata": {},
   "source": [
    "#### Accounting for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores=[]\n",
    "auc_scores=[]\n",
    "for i in range(20):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data,0.7)\n",
    "    log_reg=LogisticRegression(class_weight='balanced',max_iter=1000).fit(X_train,y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    recall_scores.append(recall_score(y_test,y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test,log_reg.predict_proba(X_test)[:,1]))\n",
    "print(\"The Logistic Regression average recall score accounting for class imbalance is: {}\".format(sum(recall_scores)/len(recall_scores)))\n",
    "print(\"The Logistic Regression average AUC score without accounting for class imbalance is: {}\".format(sum(auc_scores)/len(auc_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-productivity",
   "metadata": {},
   "source": [
    "An example of a confusion matrix in one of the 20 repetitions estimated above\n",
    "\n",
    "|Confusion Matrix|y_true=1|y_true=0|\n",
    "|----------------|--------|--------|\n",
    "|**y_predicted=1**|TP=134|FP=3,879|\n",
    "|**y_predicted=0**|FN=14|TN=81,416|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-repository",
   "metadata": {},
   "source": [
    "Accounting for class imbalance improves bot the recall score and the AUC. \n",
    "\n",
    "It is worth highlighting that the number of false positives soared (~240 times the false positives found on the model without balanced weights). We can assume that a bank incurrs in more costs when failing to detect fraudulent transactions than when labelling as fraudulent a legal transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-fields",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mounted-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "linear-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=StandardScaler().fit_transform(data.drop('Class',axis=1))\n",
    "data2=pd.DataFrame(data2,columns=data.drop('Class',axis=1).columns)\n",
    "data2=pd.concat([data2,data.Class],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funny-testimony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SVC test recall score accounting for class imbalance is: 0.7254237288135593\n",
      "The SVC train recall score accounting for class imbalance is: 0.9949238578680203\n"
     ]
    }
   ],
   "source": [
    "recall_scores_test=[]\n",
    "recall_scores_train=[]\n",
    "\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data2,0.4)\n",
    "    svc=SVC(class_weight='balanced',C=0.1).fit(X_train,y_train)\n",
    "    y_pred_test=svc.predict(X_test)\n",
    "    y_pred_train=svc.predict(X_train)\n",
    "    recall_scores_test.append(recall_score(y_test,y_pred_test))\n",
    "    recall_scores_train.append(recall_score(y_train,y_pred_train))\n",
    "print(\"The SVC test recall score accounting for class imbalance is: {}\".format(sum(recall_scores_test)/len(recall_scores_test)))\n",
    "print(\"The SVC train recall score accounting for class imbalance is: {}\".format(sum(recall_scores_train)/len(recall_scores_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-lender",
   "metadata": {},
   "source": [
    "The SVM performance with RBF kernel is not accurate if we compare it with the logistic regression results.\n",
    "\n",
    "The python package is not efficient to calculate SVM in large datasets, therefore I will use Stochastic Gradient Descent with a Linear Kernel to see if the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=list(np.arange(0.1,0.5,0.05))\n",
    "test_scores=[]\n",
    "train_scores=[]\n",
    "for i in sample_size:\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data2,i)\n",
    "    svc=SVC(class_weight='balanced').fit(X_train,y_train)\n",
    "    y_pred=svc.predict(X_test)\n",
    "    y_train_pred=svc.predict(X_train)\n",
    "    test_scores.append(recall_score(y_test,y_pred))\n",
    "    train_scores.append(recall_score(y_train,y_train_pred))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_size,train_scores)\n",
    "plt.plot(sample_size,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-update",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
