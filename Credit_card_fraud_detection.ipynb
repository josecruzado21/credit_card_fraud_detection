{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accompanied-claim",
   "metadata": {},
   "source": [
    "# Fraud Detection in Credit Card Payments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-renewal",
   "metadata": {},
   "source": [
    "The data used in the following notebook was obtained from Kaggle (Dal Pozzolo et al 2015). Link to the dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud/data\n",
    "\n",
    "According to the dictionary, to protect the identity, the variables in the data set are the consequence of a dimensionality reduction process (PCA). The time variable represent the number of seconds elapsed between the transaction and the first transaction in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-pride",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "In case you cloned this repository from Amazon Sagemaker, the best way to obtain the data is to run the following lines:\n",
    "\n",
    "```\n",
    "!rm creditcard.csv\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c534768_creditcardfraud/creditcardfraud.zip\n",
    "!unzip creditcardfraud\n",
    "!rm creditcardfraud.zip\n",
    "```\n",
    "\n",
    "Because creditcard.csv was uploaded using `lfs`, When this repository is cloned from Sagemaker, the creditcard.csv file will contain only pointers.\n",
    "\n",
    "If you clone this repo locally and `lfs` is configured, the dataset will be correcly downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-hurricane",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valued-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm creditcard.csv\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c534768_creditcardfraud/creditcardfraud.zip\n",
    "!unzip creditcardfraud\n",
    "!rm creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informative-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italic-diving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 284807 observations and 31 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The dataset contains {} observations and {} features\".format(data.shape[0],data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-heading",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "* V1-V28: The principal componentes obtained by PCA. The original features cannot be disclosed for confidentiality reasons.\n",
    "* Time: Seconds elapsed between the transaction and the first transaction in the dataset.\n",
    "* Amount: The amount of the transaction.\n",
    "* Class: Takes the value of **1** if it was a fraudulent transaction and **0** otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-measure",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "The data contains two days of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Time.plot(kind='density');\n",
    "data.Time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-likelihood",
   "metadata": {},
   "source": [
    "#### PCA components\n",
    "\n",
    "As expected, all the components of the PCA has zero correlation with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.loc[:,data.columns.str.startswith('V')].corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-reputation",
   "metadata": {},
   "source": [
    "#### Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-letters",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-strap",
   "metadata": {},
   "source": [
    "The dataset is extremely imbalanced, with only 0.17% of the observations labelled as fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of fraudulent transactions in the dataset: {}\".format(data.Class.value_counts()[1]))\n",
    "print(\"Proportion of fraudulent transactions in the dataset: {}%\".format((data.Class.value_counts()[1]/len(data.Class))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-exchange",
   "metadata": {},
   "source": [
    "## Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-jackson",
   "metadata": {},
   "source": [
    "Our task is to detect the as many fraudulent transactions as possible. Provided that our data is extremely imbalanced, with more that 99% of the observations labelled as non-fraudulent, algorithms will generally tend to predict all transactions to be on the majority class. For this reason, we should focus on minimizing False Negatives and maximizing True Positives (maximizing Recall).\n",
    "\n",
    "\\begin{align}\n",
    "Recall=\\frac{TP}{TP+FN}\n",
    "\\end{align}\n",
    "\n",
    "Performing cross validation to estimate the performance (recall) on the test set would not give accurate results. The reason for this is the low proportion of fraudulent transactions, which causes a great variability on the amout of class-1 observations used for training and test in each validation set. \n",
    "\n",
    "As we only have 492 fraudulent data points, training an algorithm with 400 of them and testing on 92 may generate very different results as training on 250 of them and testing on the rest.\n",
    "\n",
    "In order to mantain the proportion of class-1 points in the training and test set, we will perform a stratified sampling to divide the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(by='Class').sample(frac=0.7).Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "medieval-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(data,fraction):\n",
    "    \"\"\"\n",
    "    Input: the dataset to split into training and test\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    The function returns X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \"\"\"\n",
    "    train=data.iloc[data.groupby(by='Class').sample(frac=fraction).index]\n",
    "    test=data.drop(train.index,axis=0)\n",
    "    \n",
    "    X_train=train.drop('Class',axis=1)\n",
    "    X_test=test.drop('Class',axis=1)\n",
    "    y_train=train.Class\n",
    "    y_test=test.Class\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-somerset",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollow-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-express",
   "metadata": {},
   "source": [
    "#### Not accounting for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores=[]\n",
    "auc_scores=[]\n",
    "for i in range(20):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data,0.7)\n",
    "    log_reg=LogisticRegression(max_iter=1000).fit(X_train,y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    recall_scores.append(recall_score(y_test,y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test,log_reg.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"The Logistic Regression average recall score without accounting for class imbalance is: {}\".format(sum(recall_scores)/len(recall_scores)))\n",
    "print(\"The Logistic Regression averace AUC score without accounting for class imbalance is: {}\".format(sum(auc_scores)/len(auc_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-pastor",
   "metadata": {},
   "source": [
    "Is important to highlight that the 20 recall scores found above are not independent. \n",
    "\n",
    "Some training observations in the training set (or test) are the same among different repetitions. However, using 20 different iterations with only 492 class-1 observations makes highly probable that all the observations are used as training and test data points at least once. The latter occurs because we always select the same proportion of the 492 observations as training and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-relaxation",
   "metadata": {},
   "source": [
    "An example of a confusion matrix in one of the 20 repetitions estimated above\n",
    "\n",
    "|Confusion Matrix|y_true=1|y_true=0|\n",
    "|----------------|--------|--------|\n",
    "|**y_predicted=1**|TP=84|FP=16|\n",
    "|**y_predicted=0**|FN=64|TN=85,279|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-signal",
   "metadata": {},
   "source": [
    "#### Accounting for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores=[]\n",
    "auc_scores=[]\n",
    "for i in range(20):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data,0.7)\n",
    "    log_reg=LogisticRegression(class_weight='balanced',max_iter=1000).fit(X_train,y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    recall_scores.append(recall_score(y_test,y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test,log_reg.predict_proba(X_test)[:,1]))\n",
    "print(\"The Logistic Regression average recall score accounting for class imbalance is: {}\".format(sum(recall_scores)/len(recall_scores)))\n",
    "print(\"The Logistic Regression average AUC score without accounting for class imbalance is: {}\".format(sum(auc_scores)/len(auc_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-folks",
   "metadata": {},
   "source": [
    "An example of a confusion matrix in one of the 20 repetitions estimated above\n",
    "\n",
    "|Confusion Matrix|y_true=1|y_true=0|\n",
    "|----------------|--------|--------|\n",
    "|**y_predicted=1**|TP=134|FP=3,879|\n",
    "|**y_predicted=0**|FN=14|TN=81,416|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-musical",
   "metadata": {},
   "source": [
    "Accounting for class imbalance improves bot the recall score and the AUC. \n",
    "\n",
    "It is worth highlighting that the number of false positives soared (~240 times the false positives found on the model without balanced weights). We can assume that a bank incurrs in more costs when failing to detect fraudulent transactions than when labelling as fraudulent a legal transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-authentication",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "characteristic-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=StandardScaler().fit_transform(data.drop('Class',axis=1))\n",
    "data2=pd.DataFrame(data2,columns=data.drop('Class',axis=1).columns)\n",
    "data2=pd.concat([data2,data.Class],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores_test=[]\n",
    "recall_scores_train=[]\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data2,0.7)\n",
    "    svc=SVC(class_weight='balanced',C=0.1).fit(X_train,y_train)\n",
    "    y_pred_test=svc.predict(X_test)\n",
    "    y_pred_train=svc.predict(X_train)\n",
    "    recall_scores_test.append(recall_score(y_test,y_pred_test))\n",
    "    recall_scores_train.append(recall_score(y_train,y_pred_train))\n",
    "print(\"The SVC test recall score accounting for class imbalance is: {}\".format(sum(recall_scores_test)/len(recall_scores_test)))\n",
    "print(\"The SVC train recall score accounting for class imbalance is: {}\".format(sum(recall_scores_train)/len(recall_scores_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=list(np.arange(0.1,0.5,0.05))\n",
    "test_scores=[]\n",
    "train_scores=[]\n",
    "for i in sample_size:\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data2,i)\n",
    "    svc=SVC(class_weight='balanced').fit(X_train,y_train)\n",
    "    y_pred=svc.predict(X_test)\n",
    "    y_train_pred=svc.predict(X_train)\n",
    "    test_scores.append(recall_score(y_test,y_pred))\n",
    "    train_scores.append(recall_score(y_train,y_train_pred))\n",
    "plt.plot(sample_size,train_scores)\n",
    "plt.plot(sample_size,test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-kansas",
   "metadata": {},
   "source": [
    "The learning curve shows that there is room for improvement in the accuracy of our SVM algorithm if we use more training data. Given that our dataset is already large, we can conclude that the gain of having more training observations does not come from non-fraudulent data. In fact the improvement in recall comes entirely from the inclussion of more class-1 observations. The aforementioned problem can be solved with oversampling techniques, which will be tested later in this project.\n",
    "\n",
    "The SVM performance with RBF kernel is not as accurate as logistic regression and is far more computationally expensive. Using Stochastic Gradient Descent (SGD) is an alternative calculation to SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-civilian",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "black-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5 MB 17 kB/s s eta 0:00:01    |█████████████▍                  | 65.8 MB 14.8 MB/s eta 0:00:07     |█████████████████▉              | 87.8 MB 70.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from xgboost) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sharp-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exotic-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The XGBoost average train accuracy score accounting for class imbalance is: 0.9999197447884273\n",
      "The XGBoost average train recall score accounting for class imbalance is: 1.0\n",
      "The XGBoost average test recall score accounting for class imbalance is: 0.8378378378378377\n",
      "The XGBoost average test AUC score accounting for class imbalance is: 0.9826439004219062\n",
      "85284 11 21 127\n"
     ]
    }
   ],
   "source": [
    "recall_scores=[]\n",
    "auc_scores=[]\n",
    "train_acc_scores=[]\n",
    "train_recall_scores=[]\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data,0.7)\n",
    "    xgb_classifier=xgb.XGBClassifier(use_label_encoder=False,objective='binary:logistic',eval_metric='auc',scale_pos_weight=600,eta=0.1,n_estimators=100,max_depth=7, alpha=0.1, n_jobs=-1)\n",
    "    xgb_classifier.fit(X_train,y_train)\n",
    "    y_pred=xgb_classifier.predict(X_test)\n",
    "    y_train_pred=xgb_classifier.predict(X_train)\n",
    "    recall_scores.append(recall_score(y_test,y_pred))\n",
    "    train_acc_scores.append(accuracy_score(y_train,y_train_pred))\n",
    "    train_recall_scores.append(recall_score(y_train,y_train_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test,xgb_classifier.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"The XGBoost average train accuracy score accounting for class imbalance is: {}\".format(sum(train_acc_scores)/len(train_acc_scores)))\n",
    "print(\"The XGBoost average train recall score accounting for class imbalance is: {}\".format(sum(train_recall_scores)/len(train_recall_scores)))\n",
    "print(\"The XGBoost average test recall score accounting for class imbalance is: {}\".format(sum(recall_scores)/len(recall_scores)))\n",
    "print(\"The XGBoost average test AUC score accounting for class imbalance is: {}\".format(sum(auc_scores)/len(auc_scores)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-transparency",
   "metadata": {},
   "source": [
    "|Confusion Matrix|y_true=1|y_true=0|\n",
    "|----------------|--------|--------|\n",
    "|**y_predicted=1**|TP=116|FP=24|\n",
    "|**y_predicted=0**|FN=32|TN=85,271|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-heritage",
   "metadata": {},
   "source": [
    "Compared to Logistc Regression, XGBoost takes generates significantly less False Positives with the cost of a lower Recall. This model would be appropriate in case the financial institutions cannot afford to investigate each one of the thousands of potential fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-victim",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fifth-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greatest-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Random forest average train accuracy score accounting for class imbalance is: 0.9973114504123111\n",
      "The Random forest average train recall score accounting for class imbalance is: 0.8976744186046511\n",
      "The Random forest average test recall score accounting for class imbalance is: 0.8391891891891892\n",
      "The Random forest average test AUC score accounting for class imbalance is: 0.9928938041740668\n",
      "85027 268 20 128\n"
     ]
    }
   ],
   "source": [
    "recall_scores=[]\n",
    "auc_scores=[]\n",
    "train_acc_scores=[]\n",
    "train_recall_scores=[]\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test=stratified_sampling(data,0.7)\n",
    "    rf=RandomForestClassifier(n_estimators=170,max_depth=5,class_weight='balanced')\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred=rf.predict(X_test)\n",
    "    y_train_pred=rf.predict(X_train)\n",
    "    recall_scores.append(recall_score(y_test,y_pred))\n",
    "    train_acc_scores.append(accuracy_score(y_train,y_train_pred))\n",
    "    train_recall_scores.append(recall_score(y_train,y_train_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test,xgb_classifier.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"The Random forest average train accuracy score accounting for class imbalance is: {}\".format(sum(train_acc_scores)/len(train_acc_scores)))\n",
    "print(\"The Random forest average train recall score accounting for class imbalance is: {}\".format(sum(train_recall_scores)/len(train_recall_scores)))\n",
    "print(\"The Random forest average test recall score accounting for class imbalance is: {}\".format(sum(recall_scores)/len(recall_scores)))\n",
    "print(\"The Random forest average test AUC score accounting for class imbalance is: {}\".format(sum(auc_scores)/len(auc_scores)))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-banana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
